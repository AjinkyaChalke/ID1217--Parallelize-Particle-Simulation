
% TODO: A plot in log-log scale that shows that your serial and parallel codes run in O(n) time and a description of the data structures that you used to achieve it. In order to get more precise timing estimates, we recommend you to run a program at least 5 times and take the median (rather than the mean) of the simulation times.

\subsection{Linear execution}

In order to know that we have achieved the desired result of linear execution we had to plot the running time against the size of the input. To to this we made a script which automated the process and outputted vectorized plots using \texttt{gnuplot}. The data points are the median value of $20$ simultaneous runs on a computer in the data lab \texttt{s1810.it.kth.se} for the interval $10 \leq n < 2000$ with a stepping of $10$ particles.

The result for our serial program can be seen in the figure below which has a logaritmic scale, and as we can see it increases linearly.

\begin{figure}[H]
	\includegraphics{plots/serial.pdf}
	\label{serial_linear}
	\caption{Execution time plotted against input size of $n$ for the serial implementation.}
\end{figure}

Our memory-shared parallel programs also proved to have linear execution when running on one thread as can be seen in the figures below.

\begin{figure}[H]
	\includegraphics{plots/openmp.pdf}
	\label{openmp_linear}
	\caption{Execution time plotted against size of $n$ for the OpenMP implementation.}
\end{figure}
\begin{figure}[H]
	\includegraphics{plots/pthreads.pdf}
	\label{pthreads_linear}
	\caption{Execution time plotted against size of $n$ for the POSIX-threads implementation.}
\end{figure}

\subsection{Optimal parallelization}

The second part of the problem was to parallelize the code so that the running time would be $O(n) / T$, where $n$ is the number of particles and $T$ is the number of threads.

The best way to measure the level of parallelization in a program is to examine the speedup factor. This is done by running the program with a fixed number of threads for a number of times, extracting the median value of that and then dividing the time with the time it took for one thread to run. The plots of the OpenMP implementation and the POSIX-threads implementation can be seen below; and as we can see the speedup-factor is close to optimal, but not quite.

The reason for this is a number of factors. But the major factor is that in order for our grid optimization to work we must update the position of each particle inside the grid. This work runs in $O(k \cdot n)$ for every thread and in order to have optimal speed-up $k$ must be very low. However, in our case this linear work shows through already after $2$ threads. But this critical section of the code is required for our choice of design.

% TODO: Speedup plots that show how closely your parallel codes approach the
% idealized p-times speedup and a discussion on whether it is possible to do
% better.
\begin{figure}[H]
	\includegraphics{plots/openmp_speedup.pdf}
	\caption{Speedup factor calculated on the \texttt{s1810.it.kth.se} computer for the OpenMP implementation.}
\end{figure}
\begin{figure}[H]
	\includegraphics{plots/pthreads_speedup.pdf}
	\caption{Speedup factor calculated on the \texttt{s1810.it.kth.se} computer for the POSIX-threads implementation.}
\end{figure}

% TODO: Where does the time go? Consider breaking down the runtime into
% computation time, synchronization time and/or communication time. How do
% they scale with p?