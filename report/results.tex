% TODO: A plot in log-log scale that shows that your serial and parallel codes run in O(n) time and a description of the data structures that you used to achieve it. In order to get more precise timing estimates, we recommend you to run a program at least 5 times and take the median (rather than the mean) of the simulation times.

\subsection{Linear execution}

In order to know that we have achieved the desired result of linear execution we
had to plot the running time against the size of the input. To to this we made a
script which automated the process and outputted vectorized plots using
\texttt{gnuplot}. The data points are the median value of $20$ simultaneous runs
on a computer in the data lab \texttt{s1810.it.kth.se} for the interval $10 \leq
n < 2000$ with a stepping of $10$ particles.

The result for our serial program can be seen in the figure below which has a
logaritmic scale, and as we can see it increases linearly.

\begin{figure}[H]
	\includegraphics{plots/serial.pdf}
	\label{serial_linear}
	\caption{Execution time plotted against input size of $n$ for the serial implementation.}
\end{figure}

Our memory-shared parallel programs also proved to have linear execution when running on one thread as can be seen in the figures below.

\begin{figure}[H]
	\includegraphics{plots/openmp.pdf}
	\label{openmp_linear}
	\caption{Execution time plotted against size of $n$ for the OpenMP implementation.}
\end{figure}
\begin{figure}[H]
	\includegraphics{plots/pthreads.pdf}
	\label{pthreads_linear}
	\caption{Execution time plotted against size of $n$ for the POSIX-threads implementation.}
\end{figure}

\begin{figure}[H]
	\includegraphics{plots/mpi.pdf}
	\label{mpi_linear}
	\caption{Execution time plotted against size of $n$ for the MPI implementation.}
\end{figure}

\subsection{Optimal parallelization}

The second part of the problem was to parallelize the code so that the running
time would be $O(n) / T$, where $n$ is the number of particles and $T$ is the
number of threads.

The best way to measure the level of parallelization in a program is to examine
the speedup factor. This is done by running the program with a fixed number of
threads for a number of times, extracting the median value of that and then
dividing the time with the time it took for one thread to run. The plots of the
OpenMP implementation and the POSIX-threads implementation can be seen below;
and as we can see the speedup-factor is close to optimal, but not quite.

The reason for this is a number of factors. But the major factor is that in
order for our grid optimization to work we must update the position of each
particle inside the grid. This work runs in $O(k \cdot n)$ for every thread and
in order to have optimal speed-up $k$ must be very low. However, in our case
this linear work shows through already after $2$ threads. But this critical
section of the code is required for our choice of design.

% TODO: Speedup plots that show how closely your parallel codes approach the
% idealized p-times speedup and a discussion on whether it is possible to do
% better.
\begin{figure}[H]
	\includegraphics{plots/openmp_speedup.pdf}
	\caption{Speedup factor calculated on the \texttt{s1810.it.kth.se} computer for the OpenMP implementation.}
\end{figure}
\begin{figure}[H]
	\includegraphics{plots/pthreads_speedup.pdf}
	\caption{Speedup factor calculated on the \texttt{s1810.it.kth.se} computer for the POSIX-threads implementation.}
\end{figure}

% TODO: Where does the time go? Consider breaking down the runtime into
% computation time, synchronization time and/or communication time. How do
% they scale with p?

\subsection{Problems with MPI}

The MPI implementation was by far the hardest of to get efficient, sadly we
wheren't able to get it as efficient as we hoped. Message passing is by its
nature slower than shared memory. While threads using shared memory always has
the same information as all other threads, message passing has no shared memory
and must therefore send messages in order to share information, this of course
takes extra time. The breakdown (figure 8) of our MPI implementation shows that
the computation time of the movement of the particles (move and force) follows
the near optimal speedup just as well as the other implementations. But when we
run multiple processes in MPI the message passing takes about 0.4 seconds, this
is a significant fraction of the whole execution time. When running with four
processes the message passing stands for 37\% of the computation time.
Unfortunately we could not find a way to remove this overhead. The message
passing will take up a more and more significant fraction of the computation
time because the real computation will less and less time as more processes are
added.

\begin{figure}[H]
    \label{breakdown}
	\includegraphics{mpilol}
	\caption{Breakdown of what time is spent on when running the MPI
    implementation.}
\end{figure}
